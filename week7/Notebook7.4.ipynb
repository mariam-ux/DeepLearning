{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# AAI612: Deep Learning & its Applications\n",
    "\n",
    "*Notebook 7.4: Deep Learning for NLP*\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/harmanani/AAI612/blob/main/Week7/Notebook7.4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "*Adopted from NVIDIA*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Language is naturally composed of sequence data, in the form of characters in words, and words in sentences. Other examples of sequence data include stock prices and  weather data over time. Videos, while containing still images, are also sequences. Elements in the data have a relationship with what comes before and what comes after, and this fact requires a different approach.  In this lab, we will look at natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Headline Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've all seen text predictors in applications like the search bars, on cell phones, or in text editors that provide autocompletion of sentences. Many of the good text predictor models are trained on very large datasets, and take a lot of time and/or processing power to train. For this exercise, our predictor will be quite simple, but it will provide some simple exposure to language processing, sequence data, and one of the classic architecture elements used to train sequences, *recurrent neural networks* or *RNNs*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset consists of headlines from the [New York Times](https://www.nytimes.com/) newspaper over the course of several months. We'll start by reading in all the headlines from the articles. The articles are in CSV files, so we can use *pandas* to read them in.  More details regarding the dataset can be found at [Kaggle](https://www.kaggle.com/datasets/aashita/nyt-comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "If you run the Notebook on Google Colab then you would need to download the dataset `nyt_dataset` and upload it to your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArticlesApril2018.csv\n",
      "ArticlesFeb2017.csv\n",
      "ArticlesFeb2018.csv\n",
      "ArticlesJan2017.csv\n",
      "ArticlesJan2018.csv\n",
      "ArticlesMarch2017.csv\n",
      "ArticlesMarch2018.csv\n",
      "ArticlesMay2017.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8449"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/harmanani/AAI612/main/Week7/nyt_dataset/'\n",
    "\n",
    "filesnames =['ArticlesApril2018.csv', 'ArticlesFeb2017.csv', 'ArticlesFeb2018.csv', 'ArticlesJan2017.csv', 'ArticlesJan2018.csv',\n",
    "            'ArticlesMarch2017.csv', 'ArticlesMarch2018.csv', 'ArticlesMay2017.csv'\n",
    "]\n",
    "\n",
    "all_headlines = []\n",
    "for filename in filesnames:\n",
    "    if 'Articles' in filename:\n",
    "        print(filename)\n",
    "        # Read in all the data from the CSV file\n",
    "        headlines_df = pd.read_csv(url + filename)\n",
    "        # Add all of the headlines to our list\n",
    "        all_headlines.extend(list(headlines_df.headline.values))\n",
    "len(all_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our first few headlines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Former N.F.L. Cheerleaders’ Settlement Offer: $1 and a Meeting With Goodell',\n",
       " 'E.P.A. to Unveil a New Rule. Its Effect: Less Science in Policymaking.',\n",
       " 'The New Noma, Explained',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'How a Bag of Texas Dirt  Became a Times Tradition',\n",
       " 'Is School a Place for Self-Expression?',\n",
       " 'Commuter Reprogramming',\n",
       " 'Unknown',\n",
       " 'Unknown',\n",
       " 'Ford Changed Leaders, Looking for a Lift. It’s Still Looking.',\n",
       " 'Romney Failed to Win at Utah Convention, But Few Believe He’s Doomed',\n",
       " 'Chain Reaction',\n",
       " 'He Forced the Vatican to Investigate Sex Abuse. Now He’s Meeting With Pope Francis.',\n",
       " 'In Berlin, artists find a home',\n",
       " 'Unknown',\n",
       " 'The Right Stuff']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headlines[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important part of natural language processing (NLP) tasks (where computers deal with language), is processing text in a way that computers can understand it. We're going to take each of the words that appears in our dataset and represent it with a number. This will be part of a process called *tokenization*. \n",
    "\n",
    "Before we do that, we need to make sure we have good data. There are some headlines that are listed as \"Unknown\".  We don't want these items in our training set, so we'll filter them out: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7772"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all headlines with the value of \"Unknown\"\n",
    "all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
    "len(all_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take another look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Former N.F.L. Cheerleaders’ Settlement Offer: $1 and a Meeting With Goodell',\n",
       " 'E.P.A. to Unveil a New Rule. Its Effect: Less Science in Policymaking.',\n",
       " 'The New Noma, Explained',\n",
       " 'How a Bag of Texas Dirt  Became a Times Tradition',\n",
       " 'Is School a Place for Self-Expression?',\n",
       " 'Commuter Reprogramming',\n",
       " 'Ford Changed Leaders, Looking for a Lift. It’s Still Looking.',\n",
       " 'Romney Failed to Win at Utah Convention, But Few Believe He’s Doomed',\n",
       " 'Chain Reaction',\n",
       " 'He Forced the Vatican to Investigate Sex Abuse. Now He’s Meeting With Pope Francis.',\n",
       " 'In Berlin, artists find a home',\n",
       " 'The Right Stuff',\n",
       " 'Jimmy Carter Knows What North Korea Wants',\n",
       " 'The Truth Is Out There',\n",
       " 'New Jersey Ruling Could Reignite Battle Over Church-State Separation',\n",
       " 'Procrastinating',\n",
       " 'Word + Quiz: dilatory',\n",
       " 'My Life-Threatening Bout With E. Coli Food Poisoning',\n",
       " 'Choosing Brexit, a Town Yearned for Its Seafaring Past, and Muddied Its Future',\n",
       " 'A Quote Disproved']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headlines[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to remove punctuation and make our sentences all lower case, because this will make our model easier to train.  For our purposes, there is little or no difference between a line ending with \"!\" or \"?\" or whether words are capitalized, as in \"The\" or lower-case, as in \"the\".  With fewer unique tokens, our model will be easier to train. \n",
    "\n",
    "We could filter our sentences prior to tokenization, but we don't need to because this can all be done using the Keras `Tokenizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, our dataset consists of a set of headlines, each made up of a series of words. We want to give our model a way of representing those words in a way that it can understand. With tokenization, we separate a piece of text into smaller chunks (tokens), which in this case are words. Each unique word is then assigned a number, as this is a way that our model can understand the data. Keras has a class that will help us tokenize our data:\n",
    "\n",
    "```python\n",
    "tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,\n",
    "    split=' ', char_level=False, oov_token=None, document_count=0, **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "Taking a look at the [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) class in Keras, we see the default values are already set up for our use case.  The `filters` string already removes punctuation and the `lower` flag sets words to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:  11079\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Tokenize the words in our headlines\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_headlines)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print('Total words: ', total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'a': 2,\n",
       " 'to': 3,\n",
       " 'of': 4,\n",
       " 'in': 5,\n",
       " 'for': 6,\n",
       " 'and': 7,\n",
       " 'on': 8,\n",
       " 'is': 9,\n",
       " 'trump': 10,\n",
       " 'with': 11,\n",
       " 'new': 12,\n",
       " 'at': 13,\n",
       " 'how': 14,\n",
       " 'what': 15,\n",
       " 'you': 16,\n",
       " 'an': 17,\n",
       " 'from': 18,\n",
       " 'as': 19,\n",
       " 'it': 20,\n",
       " 'your': 21,\n",
       " 'are': 22,\n",
       " 'trump’s': 23,\n",
       " 'not': 24,\n",
       " 'be': 25,\n",
       " 'that': 26,\n",
       " 'i': 27,\n",
       " 's': 28,\n",
       " 'u': 29,\n",
       " 'season': 30,\n",
       " 'by': 31,\n",
       " 'about': 32,\n",
       " 'but': 33,\n",
       " 'can': 34,\n",
       " 'do': 35,\n",
       " 'episode': 36,\n",
       " 'up': 37,\n",
       " 'over': 38,\n",
       " 'when': 39,\n",
       " 'no': 40,\n",
       " 'york': 41,\n",
       " 'out': 42,\n",
       " '’': 43,\n",
       " 'more': 44,\n",
       " 'p': 45,\n",
       " 'after': 46,\n",
       " 'why': 47,\n",
       " 'this': 48,\n",
       " 'may': 49,\n",
       " 'o': 50,\n",
       " '‘the': 51,\n",
       " 'it’s': 52,\n",
       " 'will': 53,\n",
       " 'we': 54,\n",
       " 'my': 55,\n",
       " 'teaching': 56,\n",
       " 'or': 57,\n",
       " 'his': 58,\n",
       " 'war': 59,\n",
       " 'its': 60,\n",
       " 'g': 61,\n",
       " 'president': 62,\n",
       " 'who': 63,\n",
       " 'health': 64,\n",
       " 'one': 65,\n",
       " 'was': 66,\n",
       " '1': 67,\n",
       " 'questions': 68,\n",
       " 'house': 69,\n",
       " 'should': 70,\n",
       " 'get': 71,\n",
       " 'into': 72,\n",
       " 'have': 73,\n",
       " 'now': 74,\n",
       " 'life': 75,\n",
       " 'home': 76,\n",
       " 'today': 77,\n",
       " 'all': 78,\n",
       " '2': 79,\n",
       " 'don’t': 80,\n",
       " 'our': 81,\n",
       " 'has': 82,\n",
       " 'first': 83,\n",
       " 'says': 84,\n",
       " 'plan': 85,\n",
       " 'world': 86,\n",
       " 'trade': 87,\n",
       " 'he': 88,\n",
       " 'women': 89,\n",
       " 'white': 90,\n",
       " 'like': 91,\n",
       " 'so': 92,\n",
       " 'too': 93,\n",
       " 'good': 94,\n",
       " 'back': 95,\n",
       " 'big': 96,\n",
       " 'right': 97,\n",
       " '3': 98,\n",
       " 'love': 99,\n",
       " 'russia': 100,\n",
       " 'mr': 101,\n",
       " 'their': 102,\n",
       " 'her': 103,\n",
       " 'donald': 104,\n",
       " 'times': 105,\n",
       " 'time': 106,\n",
       " 'if': 107,\n",
       " 'off': 108,\n",
       " 'china': 109,\n",
       " 'go': 110,\n",
       " 'year': 111,\n",
       " 'north': 112,\n",
       " 'recap': 113,\n",
       " 'way': 114,\n",
       " 'city': 115,\n",
       " 'going': 116,\n",
       " 'america': 117,\n",
       " 'where': 118,\n",
       " 'activities': 119,\n",
       " 'just': 120,\n",
       " 'care': 121,\n",
       " 'still': 122,\n",
       " 'help': 123,\n",
       " 'they': 124,\n",
       " 'f': 125,\n",
       " 'race': 126,\n",
       " 'black': 127,\n",
       " 'what’s': 128,\n",
       " 'power': 129,\n",
       " 'two': 130,\n",
       " 'make': 131,\n",
       " 'say': 132,\n",
       " '6': 133,\n",
       " 'variety': 134,\n",
       " 'b': 135,\n",
       " 'better': 136,\n",
       " 'old': 137,\n",
       " 'gun': 138,\n",
       " 'american': 139,\n",
       " 'end': 140,\n",
       " 'could': 141,\n",
       " 'man': 142,\n",
       " 'than': 143,\n",
       " 'rules': 144,\n",
       " 'day': 145,\n",
       " 'own': 146,\n",
       " 'democrats': 147,\n",
       " 'me': 148,\n",
       " 'korea': 149,\n",
       " 'state': 150,\n",
       " 'change': 151,\n",
       " 'vs': 152,\n",
       " 'people': 153,\n",
       " 'bad': 154,\n",
       " 'news': 155,\n",
       " 'long': 156,\n",
       " 'some': 157,\n",
       " '5': 158,\n",
       " 'real': 159,\n",
       " 'c': 160,\n",
       " 'chief': 161,\n",
       " 'another': 162,\n",
       " 'deal': 163,\n",
       " 'political': 164,\n",
       " 'little': 165,\n",
       " 'school': 166,\n",
       " 'case': 167,\n",
       " 'death': 168,\n",
       " 'e': 169,\n",
       " 'food': 170,\n",
       " 'past': 171,\n",
       " 'court': 172,\n",
       " 'obama': 173,\n",
       " 'family': 174,\n",
       " 'want': 175,\n",
       " 'again': 176,\n",
       " 'then': 177,\n",
       " 'tax': 178,\n",
       " 'does': 179,\n",
       " 'take': 180,\n",
       " 'bill': 181,\n",
       " 'high': 182,\n",
       " 'against': 183,\n",
       " 'heart': 184,\n",
       " 'next': 185,\n",
       " 'great': 186,\n",
       " 'republicans': 187,\n",
       " 'takes': 188,\n",
       " 'work': 189,\n",
       " 'n': 190,\n",
       " 'years': 191,\n",
       " 'fight': 192,\n",
       " 'talk': 193,\n",
       " 'law': 194,\n",
       " '7': 195,\n",
       " '2017': 196,\n",
       " 'save': 197,\n",
       " 'behind': 198,\n",
       " 'student': 199,\n",
       " 'mind': 200,\n",
       " 'history': 201,\n",
       " 'children': 202,\n",
       " 'police': 203,\n",
       " 'would': 204,\n",
       " 'pay': 205,\n",
       " 'money': 206,\n",
       " 'picture': 207,\n",
       " 'tariffs': 208,\n",
       " 'facebook': 209,\n",
       " 'men': 210,\n",
       " 'party': 211,\n",
       " 'ex': 212,\n",
       " 'syria': 213,\n",
       " 'homes': 214,\n",
       " 'climate': 215,\n",
       " '—': 216,\n",
       " 'see': 217,\n",
       " 'battle': 218,\n",
       " 'need': 219,\n",
       " 'won’t': 220,\n",
       " 'open': 221,\n",
       " 'before': 222,\n",
       " 'think': 223,\n",
       " 'under': 224,\n",
       " 'south': 225,\n",
       " 'story': 226,\n",
       " 'wall': 227,\n",
       " 'place': 228,\n",
       " 'sex': 229,\n",
       " 'crisis': 230,\n",
       " 'justice': 231,\n",
       " 'politics': 232,\n",
       " 'can’t': 233,\n",
       " 'call': 234,\n",
       " 'much': 235,\n",
       " 'us': 236,\n",
       " 'march': 237,\n",
       " 'future': 238,\n",
       " 'down': 239,\n",
       " '2018': 240,\n",
       " 'risk': 241,\n",
       " 'republican': 242,\n",
       " 'immigration': 243,\n",
       " 'secret': 244,\n",
       " 'states': 245,\n",
       " 'here': 246,\n",
       " 'ban': 247,\n",
       " 'anti': 248,\n",
       " 'start': 249,\n",
       " '4': 250,\n",
       " 'california': 251,\n",
       " 'rise': 252,\n",
       " 'vote': 253,\n",
       " 'l': 254,\n",
       " 'win': 255,\n",
       " 'subway': 256,\n",
       " 'being': 257,\n",
       " 'dead': 258,\n",
       " 'leader': 259,\n",
       " '000': 260,\n",
       " 'office': 261,\n",
       " 'russian': 262,\n",
       " 'young': 263,\n",
       " 'last': 264,\n",
       " 'policy': 265,\n",
       " 'students': 266,\n",
       " 'really': 267,\n",
       " 'less': 268,\n",
       " 'making': 269,\n",
       " 'comey': 270,\n",
       " 'art': 271,\n",
       " 'live': 272,\n",
       " 'night': 273,\n",
       " 'without': 274,\n",
       " 'age': 275,\n",
       " 'child': 276,\n",
       " 'judge': 277,\n",
       " 'top': 278,\n",
       " 'most': 279,\n",
       " 'fix': 280,\n",
       " 'cancer': 281,\n",
       " 'left': 282,\n",
       " 'fear': 283,\n",
       " 'fire': 284,\n",
       " 'nuclear': 285,\n",
       " '8': 286,\n",
       " 'eat': 287,\n",
       " 'brooklyn': 288,\n",
       " 'security': 289,\n",
       " 'game': 290,\n",
       " 'did': 291,\n",
       " 'keep': 292,\n",
       " 'obamacare': 293,\n",
       " 'there': 294,\n",
       " 'kids': 295,\n",
       " 'let': 296,\n",
       " 'know': 297,\n",
       " 'metoo': 298,\n",
       " 'other': 299,\n",
       " 'attack': 300,\n",
       " 'him': 301,\n",
       " 'close': 302,\n",
       " 'face': 303,\n",
       " 'schools': 304,\n",
       " 'social': 305,\n",
       " 'control': 306,\n",
       " 'book': 307,\n",
       " 'test': 308,\n",
       " 'border': 309,\n",
       " 'find': 310,\n",
       " '10': 311,\n",
       " 'lead': 312,\n",
       " 'senate': 313,\n",
       " 'parents': 314,\n",
       " 'problem': 315,\n",
       " 'washington': 316,\n",
       " 'best': 317,\n",
       " 'mueller': 318,\n",
       " 'college': 319,\n",
       " 'media': 320,\n",
       " 'while': 321,\n",
       " 'gets': 322,\n",
       " 'play': 323,\n",
       " 'free': 324,\n",
       " 'act': 325,\n",
       " 'inquiry': 326,\n",
       " 'living': 327,\n",
       " 'truth': 328,\n",
       " 'stand': 329,\n",
       " 'hope': 330,\n",
       " 'them': 331,\n",
       " 'debate': 332,\n",
       " 'britain': 333,\n",
       " 'missing': 334,\n",
       " 'far': 335,\n",
       " 'look': 336,\n",
       " 'ask': 337,\n",
       " 'congress': 338,\n",
       " 'military': 339,\n",
       " 'friday': 340,\n",
       " 'star': 341,\n",
       " 'only': 342,\n",
       " 'vietnam': 343,\n",
       " 'isn’t': 344,\n",
       " 'taking': 345,\n",
       " 'use': 346,\n",
       " 'side': 347,\n",
       " 'getting': 348,\n",
       " 'global': 349,\n",
       " 'ready': 350,\n",
       " 'market': 351,\n",
       " 'million': 352,\n",
       " 'lost': 353,\n",
       " 'travel': 354,\n",
       " 'k': 355,\n",
       " 'talks': 356,\n",
       " 'dies': 357,\n",
       " 'era': 358,\n",
       " 'country': 359,\n",
       " 'budget': 360,\n",
       " 'president’s': 361,\n",
       " 'former': 362,\n",
       " 'wants': 363,\n",
       " 'rights': 364,\n",
       " 'favorite': 365,\n",
       " '‘a': 366,\n",
       " 'plans': 367,\n",
       " 'come': 368,\n",
       " 'west': 369,\n",
       " 'hard': 370,\n",
       " 'acrostic': 371,\n",
       " 'data': 372,\n",
       " 'chinese': 373,\n",
       " 'cut': 374,\n",
       " 'trial': 375,\n",
       " 'words': 376,\n",
       " 'walking': 377,\n",
       " 'election': 378,\n",
       " '…': 379,\n",
       " 'democracy': 380,\n",
       " 'second': 381,\n",
       " 'tell': 382,\n",
       " 'french': 383,\n",
       " 'lesson': 384,\n",
       " 'faces': 385,\n",
       " 'mindful': 386,\n",
       " 'science': 387,\n",
       " 'looking': 388,\n",
       " 'cuts': 389,\n",
       " 'wrong': 390,\n",
       " 'risks': 391,\n",
       " 'presidency': 392,\n",
       " 'stop': 393,\n",
       " 'de': 394,\n",
       " 'door': 395,\n",
       " 'were': 396,\n",
       " 'teenagers': 397,\n",
       " 'king': 398,\n",
       " 'supreme': 399,\n",
       " 'putin': 400,\n",
       " 'these': 401,\n",
       " 'said': 402,\n",
       " 'many': 403,\n",
       " 'building': 404,\n",
       " 'three': 405,\n",
       " 'must': 406,\n",
       " 'even': 407,\n",
       " 'action': 408,\n",
       " 'needs': 409,\n",
       " 'made': 410,\n",
       " 'turn': 411,\n",
       " 'winter': 412,\n",
       " 'abuse': 413,\n",
       " 'class': 414,\n",
       " 'cuomo': 415,\n",
       " 'set': 416,\n",
       " 'puzzle': 417,\n",
       " 'voice': 418,\n",
       " 'makes': 419,\n",
       " 'body': 420,\n",
       " 'challenge': 421,\n",
       " '’s': 422,\n",
       " 'give': 423,\n",
       " 'sale': 424,\n",
       " 'role': 425,\n",
       " 'yes': 426,\n",
       " 'teachers': 427,\n",
       " 'tv': 428,\n",
       " 'street': 429,\n",
       " 'show': 430,\n",
       " 'mailbag': 431,\n",
       " 'across': 432,\n",
       " 'shutdown': 433,\n",
       " 'town': 434,\n",
       " 'got': 435,\n",
       " 'education': 436,\n",
       " 'path': 437,\n",
       " 'becomes': 438,\n",
       " 'support': 439,\n",
       " 'shows': 440,\n",
       " 'stephen': 441,\n",
       " 'legal': 442,\n",
       " 'team': 443,\n",
       " 'things': 444,\n",
       " 'through': 445,\n",
       " 'looks': 446,\n",
       " 'red': 447,\n",
       " 'florida': 448,\n",
       " 'dream': 449,\n",
       " 'week': 450,\n",
       " 'modern': 451,\n",
       " 'fast': 452,\n",
       " 'coming': 453,\n",
       " 'put': 454,\n",
       " 'isis': 455,\n",
       " 'james': 456,\n",
       " 'pressure': 457,\n",
       " 'v': 458,\n",
       " 'safety': 459,\n",
       " 'had': 460,\n",
       " 'americans': 461,\n",
       " 'doctor': 462,\n",
       " 'tech': 463,\n",
       " 'might': 464,\n",
       " 'break': 465,\n",
       " 'i’m': 466,\n",
       " 'pain': 467,\n",
       " 'near': 468,\n",
       " 't': 469,\n",
       " 'question': 470,\n",
       " 'john': 471,\n",
       " 'sessions': 472,\n",
       " 'comes': 473,\n",
       " 'meeting': 474,\n",
       " 'self': 475,\n",
       " 'guns': 476,\n",
       " 'feel': 477,\n",
       " 'meet': 478,\n",
       " 'fake': 479,\n",
       " 'here’s': 480,\n",
       " 'air': 481,\n",
       " 'blood': 482,\n",
       " 'dept': 483,\n",
       " 'tells': 484,\n",
       " 'found': 485,\n",
       " 'israel': 486,\n",
       " 'allies': 487,\n",
       " 'crossword': 488,\n",
       " 'leave': 489,\n",
       " 'americans’': 490,\n",
       " 'run': 491,\n",
       " 'she': 492,\n",
       " 'aide': 493,\n",
       " 'meets': 494,\n",
       " 'try': 495,\n",
       " 'friends': 496,\n",
       " 'crash': 497,\n",
       " 'warm': 498,\n",
       " 'you’re': 499,\n",
       " 'been': 500,\n",
       " 'early': 501,\n",
       " 'trip': 502,\n",
       " 'dead’': 503,\n",
       " 'national': 504,\n",
       " 'jobs': 505,\n",
       " 'room': 506,\n",
       " 'finding': 507,\n",
       " '‘i': 508,\n",
       " 'slow': 509,\n",
       " 'reality': 510,\n",
       " 'fall': 511,\n",
       " 'claims': 512,\n",
       " 'idea': 513,\n",
       " 'learn': 514,\n",
       " 'woman': 515,\n",
       " 'job': 516,\n",
       " 'texas': 517,\n",
       " 'leaders': 518,\n",
       " 'he’s': 519,\n",
       " 'jimmy': 520,\n",
       " 'cold': 521,\n",
       " 'push': 522,\n",
       " 'key': 523,\n",
       " 'd': 524,\n",
       " 'm': 525,\n",
       " 'post': 526,\n",
       " 'europe': 527,\n",
       " 'never': 528,\n",
       " 'pick': 529,\n",
       " 'once': 530,\n",
       " 'doesn’t': 531,\n",
       " 'oil': 532,\n",
       " 'drug': 533,\n",
       " 'list': 534,\n",
       " 'threat': 535,\n",
       " 'pregnancy': 536,\n",
       " 'rising': 537,\n",
       " 'privacy': 538,\n",
       " 'met': 539,\n",
       " 'america’s': 540,\n",
       " 'advice': 541,\n",
       " 'loss': 542,\n",
       " 'co': 543,\n",
       " 'weight': 544,\n",
       " 'matter': 545,\n",
       " 'r': 546,\n",
       " 'island': 547,\n",
       " 'paid': 548,\n",
       " 'dear': 549,\n",
       " 'bannon': 550,\n",
       " 'lives': 551,\n",
       " 'voters': 552,\n",
       " 'small': 553,\n",
       " 'readers': 554,\n",
       " 'well': 555,\n",
       " 'sports': 556,\n",
       " 'fighting': 557,\n",
       " 'price': 558,\n",
       " 'away': 559,\n",
       " 'memory': 560,\n",
       " 'mother': 561,\n",
       " 'goes': 562,\n",
       " 'low': 563,\n",
       " 'success': 564,\n",
       " 'united': 565,\n",
       " 'worth': 566,\n",
       " 'aid': 567,\n",
       " 'word': 568,\n",
       " 'human': 569,\n",
       " 'losing': 570,\n",
       " 'beyond': 571,\n",
       " 'ethics': 572,\n",
       " 'inside': 573,\n",
       " 'space': 574,\n",
       " 'officials': 575,\n",
       " 'safe': 576,\n",
       " 'tied': 577,\n",
       " 'economy': 578,\n",
       " 'others': 579,\n",
       " 'road': 580,\n",
       " 'dr': 581,\n",
       " 'syrian': 582,\n",
       " 'ryan': 583,\n",
       " 'wish': 584,\n",
       " 'force': 585,\n",
       " 'that’s': 586,\n",
       " 'hopes': 587,\n",
       " 'report': 588,\n",
       " 'spring': 589,\n",
       " 'view': 590,\n",
       " 'learning': 591,\n",
       " 'target': 592,\n",
       " 'match': 593,\n",
       " 'spy': 594,\n",
       " 'raising': 595,\n",
       " '15': 596,\n",
       " 'labor': 597,\n",
       " 'beat': 598,\n",
       " 'manhattan': 599,\n",
       " 'scandal': 600,\n",
       " 'silence': 601,\n",
       " 'attacks': 602,\n",
       " 'video': 603,\n",
       " 'hollywood': 604,\n",
       " 'shooting': 605,\n",
       " 'search': 606,\n",
       " 'moment': 607,\n",
       " 'girls': 608,\n",
       " 'defense': 609,\n",
       " 'workers': 610,\n",
       " 'fed': 611,\n",
       " 'review': 612,\n",
       " 'happy': 613,\n",
       " 'raise': 614,\n",
       " 'speech': 615,\n",
       " 'ways': 616,\n",
       " 'hot': 617,\n",
       " 'turns': 618,\n",
       " 'gone': 619,\n",
       " 'economic': 620,\n",
       " 'music': 621,\n",
       " 'business': 622,\n",
       " 'drag': 623,\n",
       " 'four': 624,\n",
       " 'charge': 625,\n",
       " 'fish': 626,\n",
       " 'hours': 627,\n",
       " 'step': 628,\n",
       " 'now’': 629,\n",
       " 'fashion': 630,\n",
       " 'didn’t': 631,\n",
       " 'late': 632,\n",
       " 'france': 633,\n",
       " 'spending': 634,\n",
       " 'press': 635,\n",
       " 'move': 636,\n",
       " 'abortion': 637,\n",
       " 'governor': 638,\n",
       " 'fired': 639,\n",
       " 'die': 640,\n",
       " 'bar': 641,\n",
       " 'fans': 642,\n",
       " 'fears': 643,\n",
       " 'center': 644,\n",
       " '9': 645,\n",
       " 'blame': 646,\n",
       " 'hate': 647,\n",
       " 'porn': 648,\n",
       " 'liberal': 649,\n",
       " 'broken': 650,\n",
       " 'sick': 651,\n",
       " 'women’s': 652,\n",
       " 'immigrants': 653,\n",
       " 'quiet': 654,\n",
       " 'chaos': 655,\n",
       " 'between': 656,\n",
       " 'edge': 657,\n",
       " 'freedom': 658,\n",
       " 'return': 659,\n",
       " 'watch': 660,\n",
       " 'both': 661,\n",
       " 'russians': 662,\n",
       " 'study': 663,\n",
       " 'system': 664,\n",
       " 'common': 665,\n",
       " 'ends': 666,\n",
       " 'repeal': 667,\n",
       " 'memo': 668,\n",
       " 'millions': 669,\n",
       " 'olympics': 670,\n",
       " 'investors': 671,\n",
       " 'lose': 672,\n",
       " 'choice': 673,\n",
       " 'easy': 674,\n",
       " 'cities': 675,\n",
       " 'contest': 676,\n",
       " 'part': 677,\n",
       " 'offer': 678,\n",
       " 'effect': 679,\n",
       " 'few': 680,\n",
       " 'believe': 681,\n",
       " 'jersey': 682,\n",
       " '‘billions’': 683,\n",
       " 'kind': 684,\n",
       " 'killing': 685,\n",
       " 'familiar': 686,\n",
       " '‘rupaul’s': 687,\n",
       " 'colbert': 688,\n",
       " 'nation': 689,\n",
       " 'billion': 690,\n",
       " 'fine': 691,\n",
       " 'gave': 692,\n",
       " 'name': 693,\n",
       " 'tests': 694,\n",
       " 'working': 695,\n",
       " 'yet': 696,\n",
       " 'housing': 697,\n",
       " 'strikes': 698,\n",
       " 'longer': 699,\n",
       " 'lot': 700,\n",
       " 'india': 701,\n",
       " 'taste': 702,\n",
       " 'line': 703,\n",
       " 'noah': 704,\n",
       " 'become': 705,\n",
       " 'car': 706,\n",
       " 'china’s': 707,\n",
       " 'tale': 708,\n",
       " 'energy': 709,\n",
       " 'disaster': 710,\n",
       " 'brain': 711,\n",
       " 'hit': 712,\n",
       " 'online': 713,\n",
       " 'very': 714,\n",
       " 'head': 715,\n",
       " 'gives': 716,\n",
       " 'something': 717,\n",
       " 'baby': 718,\n",
       " 'limits': 719,\n",
       " 'eye': 720,\n",
       " 'mean': 721,\n",
       " 'sees': 722,\n",
       " 'grows': 723,\n",
       " 'jail': 724,\n",
       " 'valley': 725,\n",
       " 'leads': 726,\n",
       " 'streets': 727,\n",
       " 'super': 728,\n",
       " 'dog': 729,\n",
       " 'enough': 730,\n",
       " 'point': 731,\n",
       " 'storm': 732,\n",
       " 'birth': 733,\n",
       " 'gold': 734,\n",
       " 'wins': 735,\n",
       " 'phone': 736,\n",
       " 'deadly': 737,\n",
       " 'killed': 738,\n",
       " 'presidential': 739,\n",
       " 'there’s': 740,\n",
       " 'welcome': 741,\n",
       " 'short': 742,\n",
       " 'ride': 743,\n",
       " 'always': 744,\n",
       " 'charges': 745,\n",
       " 'hits': 746,\n",
       " 'service': 747,\n",
       " '13': 748,\n",
       " 'hall': 749,\n",
       " 'thing': 750,\n",
       " 'alienist’': 751,\n",
       " 'drugs': 752,\n",
       " 'quiz': 753,\n",
       " 'iran': 754,\n",
       " 'ever': 755,\n",
       " 'heck': 756,\n",
       " 'trying': 757,\n",
       " 'let’s': 758,\n",
       " 'special': 759,\n",
       " 'public': 760,\n",
       " 'wonkish': 761,\n",
       " 'turning': 762,\n",
       " 'finally': 763,\n",
       " 'calling': 764,\n",
       " 'took': 765,\n",
       " 'who’s': 766,\n",
       " 'shift': 767,\n",
       " 'simple': 768,\n",
       " 'homeless': 769,\n",
       " 'trevor': 770,\n",
       " 'cash': 771,\n",
       " 'details': 772,\n",
       " 'kitchen': 773,\n",
       " 'exercise': 774,\n",
       " 'toll': 775,\n",
       " 'wait': 776,\n",
       " '‘how': 777,\n",
       " 'calls': 778,\n",
       " 'speak': 779,\n",
       " 'we’re': 780,\n",
       " 'trail': 781,\n",
       " '‘here': 782,\n",
       " 'puts': 783,\n",
       " 'god': 784,\n",
       " 'stress': 785,\n",
       " 'gay': 786,\n",
       " 'walk': 787,\n",
       " 'shot': 788,\n",
       " 'maybe': 789,\n",
       " 'assassination': 790,\n",
       " 'silicon': 791,\n",
       " 'light': 792,\n",
       " 'secrets': 793,\n",
       " 'government': 794,\n",
       " 'stay': 795,\n",
       " 'protests': 796,\n",
       " 'sleep': 797,\n",
       " 'peace': 798,\n",
       " 'lies': 799,\n",
       " 'bring': 800,\n",
       " 'abroad': 801,\n",
       " 'la': 802,\n",
       " 'photos': 803,\n",
       " 'refugee': 804,\n",
       " 'guilty': 805,\n",
       " 'same': 806,\n",
       " 'growth': 807,\n",
       " 'fresh': 808,\n",
       " 'sets': 809,\n",
       " 'lessons': 810,\n",
       " 'days': 811,\n",
       " 'pass': 812,\n",
       " 'jan': 813,\n",
       " 'read': 814,\n",
       " 'kushner': 815,\n",
       " 'hands': 816,\n",
       " 'soul': 817,\n",
       " 'sound': 818,\n",
       " 'you’ve': 819,\n",
       " 'paris': 820,\n",
       " 'train': 821,\n",
       " 'decision': 822,\n",
       " 'cover': 823,\n",
       " 'turkey': 824,\n",
       " 'trouble': 825,\n",
       " 'chef': 826,\n",
       " 'everyone': 827,\n",
       " 'knows': 828,\n",
       " 'gap': 829,\n",
       " 'arizona': 830,\n",
       " 'paul': 831,\n",
       " 'nothing': 832,\n",
       " 'lower': 833,\n",
       " 'movie': 834,\n",
       " 'robots': 835,\n",
       " 'technology': 836,\n",
       " 'magic': 837,\n",
       " 'middle': 838,\n",
       " 'ideas': 839,\n",
       " 'blasio': 840,\n",
       " 'deals': 841,\n",
       " 'doing': 842,\n",
       " 'pruitt': 843,\n",
       " 'democratic': 844,\n",
       " 'seen': 845,\n",
       " 'director': 846,\n",
       " 'marriage': 847,\n",
       " 'trust': 848,\n",
       " 'promise': 849,\n",
       " 'sanctions': 850,\n",
       " 'flight': 851,\n",
       " 'sea': 852,\n",
       " 'remember': 853,\n",
       " 'ahead': 854,\n",
       " 'stars': 855,\n",
       " 'issue': 856,\n",
       " 'begins': 857,\n",
       " 'which': 858,\n",
       " 'major': 859,\n",
       " 'worst': 860,\n",
       " 'embrace': 861,\n",
       " 'classic': 862,\n",
       " 'civil': 863,\n",
       " 'angry': 864,\n",
       " 'course': 865,\n",
       " 'votes': 866,\n",
       " 'gender': 867,\n",
       " 'vows': 868,\n",
       " 'teach': 869,\n",
       " 'boss': 870,\n",
       " 'moves': 871,\n",
       " 'every': 872,\n",
       " 'york’s': 873,\n",
       " 'tiny': 874,\n",
       " 'kill': 875,\n",
       " 'google': 876,\n",
       " 'stock': 877,\n",
       " 'amazon': 878,\n",
       " 'land': 879,\n",
       " 'legacy': 880,\n",
       " 'crime': 881,\n",
       " 'month': 882,\n",
       " 'son': 883,\n",
       " 'cost': 884,\n",
       " 'hero': 885,\n",
       " 'harassment': 886,\n",
       " 'they’re': 887,\n",
       " 'driving': 888,\n",
       " 'tune': 889,\n",
       " 'dirty': 890,\n",
       " 'fury': 891,\n",
       " 'doctors': 892,\n",
       " 'add': 893,\n",
       " 'finale': 894,\n",
       " 'terror': 895,\n",
       " 'secretary': 896,\n",
       " 'arms': 897,\n",
       " 'macron': 898,\n",
       " 'led': 899,\n",
       " 'unlikely': 900,\n",
       " 'tries': 901,\n",
       " 'comments': 902,\n",
       " 'op': 903,\n",
       " 'water': 904,\n",
       " 'clinton': 905,\n",
       " 'mayor': 906,\n",
       " 'bridge': 907,\n",
       " 'clues': 908,\n",
       " 'intelligence': 909,\n",
       " 'blue': 910,\n",
       " 'transgender': 911,\n",
       " 'waiting': 912,\n",
       " 'rate': 913,\n",
       " 'feb': 914,\n",
       " 'sweet': 915,\n",
       " 'speed': 916,\n",
       " 'falling': 917,\n",
       " 'campaign': 918,\n",
       " 'feud': 919,\n",
       " 'reporter': 920,\n",
       " 'federal': 921,\n",
       " 'ally': 922,\n",
       " 'likely': 923,\n",
       " 'hear': 924,\n",
       " 'dark': 925,\n",
       " 'clash': 926,\n",
       " 'order': 927,\n",
       " 'remain': 928,\n",
       " 'madness': 929,\n",
       " 'pope': 930,\n",
       " 'artists': 931,\n",
       " 'tragedy': 932,\n",
       " 'overlooked': 933,\n",
       " 'dinner': 934,\n",
       " 'canada': 935,\n",
       " 'facing': 936,\n",
       " 'families': 937,\n",
       " 'message': 938,\n",
       " 'told': 939,\n",
       " 'choose': 940,\n",
       " 'premiere': 941,\n",
       " 'wild': 942,\n",
       " 'sorry': 943,\n",
       " 'gaza': 944,\n",
       " 'voting': 945,\n",
       " 'east': 946,\n",
       " 'race’': 947,\n",
       " '50': 948,\n",
       " 'testing': 949,\n",
       " 'candidates': 950,\n",
       " 'marijuana': 951,\n",
       " 'reading': 952,\n",
       " 'returns': 953,\n",
       " 'fraud': 954,\n",
       " 'deputy': 955,\n",
       " 'restaurant': 956,\n",
       " 'bias': 957,\n",
       " 'fit': 958,\n",
       " 'changes': 959,\n",
       " 'interview': 960,\n",
       " 'around': 961,\n",
       " 'running': 962,\n",
       " 'genius': 963,\n",
       " 'math': 964,\n",
       " 'leaves': 965,\n",
       " 'rises': 966,\n",
       " 'animals': 967,\n",
       " 'childhood': 968,\n",
       " 'mexico': 969,\n",
       " 'beef': 970,\n",
       " 'alternative': 971,\n",
       " 'murder': 972,\n",
       " 'doors': 973,\n",
       " 'protest': 974,\n",
       " 'during': 975,\n",
       " 'affair': 976,\n",
       " 'built': 977,\n",
       " 'steps': 978,\n",
       " 'italy': 979,\n",
       " 'those': 980,\n",
       " 'defying': 981,\n",
       " 'ice': 982,\n",
       " 'adviser': 983,\n",
       " 'hair': 984,\n",
       " 'afghan': 985,\n",
       " 'journalism': 986,\n",
       " 'later': 987,\n",
       " 'missile': 988,\n",
       " 'books': 989,\n",
       " 'tower': 990,\n",
       " 'diet': 991,\n",
       " 'forces': 992,\n",
       " 'strength': 993,\n",
       " 'corruption': 994,\n",
       " 'radical': 995,\n",
       " 'knew': 996,\n",
       " 'king’s': 997,\n",
       " 'kimmel': 998,\n",
       " 'hand': 999,\n",
       " 'healthy': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a quick look at the `word_index` dictionary to see how the tokenizer saves the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 2, 'plan': 85, 'man': 142, 'panama': 3124, 'canal': 6225}\n"
     ]
    }
   ],
   "source": [
    "# Print a subset of the word_index dictionary created by Tokenizer\n",
    "subset_dict = {key: value for key, value in tokenizer.word_index.items() \\\n",
    "               if key in ['a','man','a','plan','a','canal','panama']}\n",
    "print(subset_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `texts_to_sequences` method to see how the tokenizer saves the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2], [142], [2], [85], [2], [6225], [3124]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['a','man','a','plan','a','canal','panama'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tokenized the data, turning each word into a representative number, we will create sequences of tokens from the headlines. These sequences are what we will train our deep learning model on. \n",
    "\n",
    "For example, let's take the headline, \"nvidia launches ray tracing gpus\". Each word is going to be replaced by a corresponding number, for instance: nvidia - 5, launches - 22, ray - 94, tracing - 16, gpus - 102. The full sequence would be: [5, 22, 94, 16, 102]. However, it is also valuable to train on the smaller sequences within the headline, such as \"nvidia launches\". We'll take each headline and create a set of sequences to fill our dataset. Next, let's use our tokenizer to convert our headlines to a set of sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['former n', 'former n f', 'former n f l', 'former n f l cheerleaders’', 'former n f l cheerleaders’ settlement']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[362, 190],\n",
       " [362, 190, 125],\n",
       " [362, 190, 125, 254],\n",
       " [362, 190, 125, 254, 4917],\n",
       " [362, 190, 125, 254, 4917, 4918]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert data to sequence of tokens \n",
    "input_sequences = []\n",
    "for line in all_headlines:\n",
    "    # Convert our headline into a sequence of tokens\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    \n",
    "    # Create a series of sequences for each headline\n",
    "    for i in range(1, len(token_list)):\n",
    "        partial_sequence = token_list[:i+1]\n",
    "        input_sequences.append(partial_sequence)\n",
    "\n",
    "print(tokenizer.sequences_to_texts(input_sequences[:5]))\n",
    "input_sequences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now our sequences are of various lengths. For our model to be able to train on the data, we need to make all the sequences the same length. To do this we'll add padding to the sequences. Keras has a built-in `pad_sequences` [method](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) that we can use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       362, 190], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Determine max sequence length\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "# Pad all sequences with zeros at the beginning to make them all max length\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "input_sequences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Predictors and Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to split up our sequences into predictors and a target. The last word of the sequence will be our target, and the first words of the sequence will be our predictors. As an example, take a look at the full headline: \"nvidia releases ampere graphics cards\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr><td>PREDICTORS </td> <td>           TARGET </td></tr>\n",
    "<tr><td>nvidia                   </td> <td>  releases </td></tr>\n",
    "<tr><td>nvidia releases               </td> <td>  ampere </td></tr>\n",
    "<tr><td>nvidia releases ampere      </td> <td>  graphics</td></tr>\n",
    "<tr><td>nvidia releases ampere graphics </td> <td>  cards</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 190,  125,  254, 4917, 4918], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictors are every word except the last\n",
    "predictors = input_sequences[:,:-1]\n",
    "# Labels are the last word\n",
    "labels = input_sequences[:,-1]\n",
    "labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like our earlier sections, these targets are categorical. We are predicting one word out of our possible total vocabulary. Instead of the network predicting scalar numbers, we will have it predict binary categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import utils\n",
    "\n",
    "labels = utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our model, we're going to use a couple of new layers to deal with our sequential data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Embedding Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first layer is an embedding layer:\n",
    "\n",
    "```Python\n",
    "model.add(Embedding(input_dimension, output_dimension, input_length=input_len))\n",
    "```\n",
    "\n",
    "This layer will take the tokenized sequences and will learn an embedding for all of the words in the training dataset. Mathematically, embeddings work the same way as a neuron in a neural network, but conceptually, their goal is to reduce the number of dimensions for some or all of the features. In this case, it will represent each word as a vector, and the information within that vector will contain the relationships between each word.\n",
    "\n",
    "Learn more about embedding layers [here](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/).\n",
    "\n",
    "<img src=\"./images/embedding.png\" style=\"width: 300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Long Short Term Memory Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next, and very important layer, is a long short term memory layer (LSTM). An LSTM is a type of recurrent neural network or RNN. Unlike traditional feed-forward networks that we've seen so far, recurrent networks have loops in them, allowing information to persist. Here's a representation of a recurrent network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rnn_rolled.png\" style=\"width: 150px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New information (x) gets passed in to the network, which spits out a prediction (h). Additionally, information from that layer gets saved, and used as input for the next prediction. This may seem a bit complicated, but let's look at it unrolled: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rnn_unrolled.png\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that when a new piece of data (x) is fed into the network, that network both spits out a prediction (h) and also passes some information along to the next layer. That next layer gets another piece of data, but gets to learn from the layer before it as well.\n",
    "\n",
    "Traditional RNNs suffer from the issue of more recent information contributing more than information from further back. LSTMs are a special type of recurrent layer that are able to learn and retain longer term information. If you'd like to read more about RNNs and LSTMs, we recommend [this article](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). \n",
    "\n",
    "Alright, let's create our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Input is max sequence length - 1, as we've removed the last word for the label\n",
    "input_len = max_sequence_len - 1 \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add input embedding layer\n",
    "model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "\n",
    "# Add LSTM layer with 100 units\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "# Add output layer\n",
    "model.add(Dense(total_words, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_9 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we compile our model with categorical crossentropy, as we're categorically predicting one word from our total vocabulary. In this case, we are not going to use accuracy as a metric, because text prediction is not measured as being more or less accurate in the same way as image classification.\n",
    "\n",
    "We are also going to select a particular optimizer that is well suited for LSTM tasks, called the *Adam* optimizer. Details of optimizers are a bit out of scope for this course, but what's important to know is that different optimizers can be better for different deep learning tasks. You can read more about them, including the Adam optimizer [here](https://medium.com/datadriveninvestor/overview-of-different-optimizers-for-neural-networks-e0ed119440c3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to earlier sections, we fit our model in the same way. This time we'll train for 30 epochs, which will take a few minutes. You'll notice that we don't have a training or validation accuracy score in this case. This reflects our different problem of text prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 39ms/step - loss: 8.0365\n",
      "Epoch 2/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 40ms/step - loss: 7.4587\n",
      "Epoch 3/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 34ms/step - loss: 7.3067\n",
      "Epoch 4/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 35ms/step - loss: 7.0695\n",
      "Epoch 5/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 34ms/step - loss: 6.8394\n",
      "Epoch 6/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 34ms/step - loss: 6.5883\n",
      "Epoch 7/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 36ms/step - loss: 6.3545\n",
      "Epoch 8/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 37ms/step - loss: 6.1109\n",
      "Epoch 9/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 40ms/step - loss: 5.9200\n",
      "Epoch 10/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 41ms/step - loss: 5.6708\n",
      "Epoch 11/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 41ms/step - loss: 5.4649\n",
      "Epoch 12/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 41ms/step - loss: 5.2729\n",
      "Epoch 13/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 42ms/step - loss: 5.0670\n",
      "Epoch 14/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 42ms/step - loss: 4.8770\n",
      "Epoch 15/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 42ms/step - loss: 4.7028\n",
      "Epoch 16/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 42ms/step - loss: 4.5438\n",
      "Epoch 17/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 43ms/step - loss: 4.3712\n",
      "Epoch 18/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 42ms/step - loss: 4.2239\n",
      "Epoch 19/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 42ms/step - loss: 4.0906\n",
      "Epoch 20/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 42ms/step - loss: 3.9502\n",
      "Epoch 21/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 42ms/step - loss: 3.8405\n",
      "Epoch 22/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 39ms/step - loss: 3.7071\n",
      "Epoch 23/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 39ms/step - loss: 3.6186\n",
      "Epoch 24/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 39ms/step - loss: 3.5038\n",
      "Epoch 25/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 39ms/step - loss: 3.4127\n",
      "Epoch 26/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 39ms/step - loss: 3.3263\n",
      "Epoch 27/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 39ms/step - loss: 3.2286\n",
      "Epoch 28/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 39ms/step - loss: 3.1520\n",
      "Epoch 29/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 39ms/step - loss: 3.0593\n",
      "Epoch 30/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 38ms/step - loss: 2.9990\n",
      "Epoch 31/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 39ms/step - loss: 2.9119\n",
      "Epoch 32/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 39ms/step - loss: 2.8749\n",
      "Epoch 33/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 39ms/step - loss: 2.8312\n",
      "Epoch 34/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 39ms/step - loss: 2.7501\n",
      "Epoch 35/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 39ms/step - loss: 2.6930\n",
      "Epoch 36/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 39ms/step - loss: 2.6471\n",
      "Epoch 37/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 39ms/step - loss: 2.5984\n",
      "Epoch 38/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 38ms/step - loss: 2.5498\n",
      "Epoch 39/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 39ms/step - loss: 2.4821\n",
      "Epoch 40/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 39ms/step - loss: 2.4495\n",
      "Epoch 41/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 39ms/step - loss: 2.4105\n",
      "Epoch 42/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 39ms/step - loss: 2.3670\n",
      "Epoch 43/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 39ms/step - loss: 2.3432\n",
      "Epoch 44/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 39ms/step - loss: 2.3047\n",
      "Epoch 45/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 39ms/step - loss: 2.2699\n",
      "Epoch 46/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 38ms/step - loss: 2.2467\n",
      "Epoch 47/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 39ms/step - loss: 2.1994\n",
      "Epoch 48/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 39ms/step - loss: 2.1680\n",
      "Epoch 49/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 39ms/step - loss: 2.1231\n",
      "Epoch 50/50\n",
      "\u001b[1m1512/1512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 40ms/step - loss: 2.0949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f1487d56d0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(predictors, labels, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the loss decreased over the course of training. We could train our model further to decrease the loss, but that would take some time, and we're not looking for a perfect text predictor right now. Next let's try using the model to make predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make predictions, we'll need to start with a seed text, and prepare it in the same way we prepared our dataset. This will mean tokenizing and padding. Once we do this, we can pass it into our model to make a prediction. We'll create a function to do this called `predict_next_token`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token(seed_text):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "     # Use `predict()` instead of `predict_classes()`\n",
    "    predictions = model.predict(token_list, verbose=0)\n",
    "\n",
    "    # Get the index of the most probable word\n",
    "    predicted_index = np.argmax(predictions, axis=-1)[0]  \n",
    "\n",
    "    return predicted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(5)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = predict_next_token(\"today in new york\")\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our tokenizer to decode the predicted word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_word = tokenizer.index_word.get(prediction, \"<UNK>\")\n",
    "predicted_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate New Headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're able to predict new words, let's create a function that can predict headlines of more than just one word. The function below creates a new headline of arbitrary length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_headline(seed_text, next_words=1):\n",
    "    for _ in range(next_words):\n",
    "        # Predict next token\n",
    "        prediction = predict_next_token(seed_text)\n",
    "        # Convert token to word\n",
    "        next_word = tokenizer.index_word.get(prediction, \"<UNK>\") \n",
    "        # Add next word to the headline. This headline will be used in the next pass of the loop.\n",
    "        seed_text += \" \" + next_word\n",
    "    # Return headline as title-case\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try some headlines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Washington Dc Is Facing Fury And The U S Truce Fuels Fear Is Fear It Was It Was It Movies Supreme The Kind\n",
      "Today In New York In A Kabul For Celebrating A Midterms Cause For The Parkland China For The United States ’ For A Civil\n",
      "The School District Has The Ruler’S Ear With A Clean Days In A Private Opinion A Lesson Plan What Dr Fifth Super Bowl’ Stories\n",
      "Crime Has Become A Team From A Young A View Of Speeding Conflict Is Sure At 88 Of Doors Areas Journalist Today For\n"
     ]
    }
   ],
   "source": [
    "seed_texts = [\n",
    "    'washington dc is',\n",
    "    'today in new york',\n",
    "    'the school district has',\n",
    "    'crime has become']\n",
    "for seed in seed_texts:\n",
    "    print(generate_headline(seed, next_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results may be a bit underwhelming after 30 epochs of training.  We can notice that most of the headlines make some kind of grammatical sense, but don't necessarily indicate a good contextual understanding.  The results might improve somewhat by running more epochs. You can do this by runnning the training `fit` cell again (and again!) to train another 30 epochs each time.  You should see the loss value go down.  Then try the tests again.  Results can vary quite a bit!\n",
    "\n",
    "Other improvements would be to try using pretrained embeddings with Word2Vec or GloVe, rather than learning them during training as we did with the Keras Embedding layer.  Some information on how to do that can be found [here](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html).\n",
    "\n",
    "Ultimately, however, NLP has moved beyond simple LSTM models to Transformer-based pre-trained models, which are able to learn language context from huge amounts of textual data such as Wikipedia.  These pre-trained models are then used as a starting point for transfer learning to solve NLP tasks such as the one we just tried for text completion.  You can try one of these models yourself by checking out this [state-of-the-art text predictor here](https://transformer.huggingface.co/doc/gpt2-large) based on a [GPT-2 model](https://openai.com/blog/better-language-models/).\n",
    "\n",
    "To learn more about Transformer-based models, check out [this blog](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) on Bidirectional Encoder Representations from Transformers (BERT) and look for information on additional coursework in the \"Next Steps\" page for this DLI course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! You've successfully trained a model to predict words in a headline, and used that model to create headlines of various lengths. Feel free to experiment and generate some more headlines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
